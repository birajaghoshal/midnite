{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability-framework - Uncertainty Tutorial 1\n",
    "\n",
    "in this notebook you will learn the intuition behind the features of the interpretability framework and how to us them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example with AlexNet\n",
    "\n",
    "Demonstration of an uncertainty measurement of an image classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Sequential\n",
    "\n",
    "import data_utils\n",
    "from interpretability_framework import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "\n",
    "Load the model, which should be evaluated.\n",
    "\n",
    "**Note: Monte Carlo Dropout ensembles only achieve proper results when used on a net which was trained with dropout. So check if the model you would like to use has dropout layers active in training.**\n",
    "\n",
    "In our example we use a pretrained AlexNet for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.classifier.add_module(\"softmax\", Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "\n",
    "Wrap your net with an PredictionEnsemble layer. This layer collects an ensemble of predictions with Monte Carlo dropout. This ensemble will be used for measuring uncertainties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = Sequential(\n",
    "        modules.PredictionEnsemble(inner=alexnet), modules.ConfidenceMeanPrediction()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "\n",
    "Prepare the ensemble layer by entering evaluation mode, to specify, that we are in test mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): PredictionEnsemble(\n",
       "    (inner): AlexNet(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "        (1): ReLU(inplace)\n",
       "        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (4): ReLU(inplace)\n",
       "        (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU(inplace)\n",
       "        (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU(inplace)\n",
       "        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace)\n",
       "        (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "      (classifier): Sequential(\n",
       "        (0): Dropout(p=0.5)\n",
       "        (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Dropout(p=0.5)\n",
       "        (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (5): ReLU(inplace)\n",
       "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ConfidenceMeanPrediction()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: \n",
    "\n",
    "Make dropout layers re-active to use predictive dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in list(alexnet.modules()):\n",
    "    if isinstance(layer, Dropout):\n",
    "        layer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: \n",
    "\n",
    "You can evaluate the predictive uncertainties by loading an image from a path or a batch of a dataset.\n",
    "data.utils transforms the images, as AlexNet needs examples with dimensions ( 1 x 3 x 227 x 227 ).\n",
    "\n",
    "\n",
    "**Option 1: use batch data** \n",
    "\n",
    "We can retrieve a batch of data from a dataset by using the pytorch dataloader implemented in data.utils.py.\n",
    "Let's specify a batch size of 5 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently no data available!\n",
    "# batch = load_imagenet_dataset(\"../data/imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: use image from path**\n",
    "\n",
    "Load an image example, for which the uncertainties should be calculated.\n",
    "\n",
    "*In order to interpret the results, we want to compare the uncertainties of an in-distribution and an out-of-distribution example.* \n",
    "As AlexNet was trained with ImageNet data, we pick an ImageNet example as an in-distribution target, for which predictions should be very confident. An out-of-distribution target is an image, which depicts something, which is not a class label of ImageNet. We use Cholitas for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_example = data_utils.get_example_from_path(\"../data/imagenet_example.jpg\")\n",
    "ood_example = data_utils.get_example_from_path(\"../data/ood_example.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: generate a random example**\n",
    "\n",
    "Retrieve a random example normalized with the distribution mean and standard deviation, which can also be seen as an out-of-distribution sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_example = data_utils.get_random_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: \n",
    "Calculate uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainties of in-distribution example\n",
      "    mean prediction: 283, class probability: 0.9980050921440125\n",
      "    total predictive entropy: 0.0186183862388134\n",
      "    total mutual information: 0.0022695695515722036\n",
      "    variational ratio: 0.001994907855987549\n"
     ]
    }
   ],
   "source": [
    "pred, pred_entropy, mutual_info, var_ratio = ensemble(id_example)\n",
    "\n",
    "print(\"Uncertainties of in-distribution example\")\n",
    "print(f\"    mean prediction: {pred.argmax()}, class probability: {pred.max()}\")\n",
    "print(f\"    total predictive entropy: {pred_entropy.sum()}\")\n",
    "print(f\"    total mutual information: {mutual_info.sum()}\")\n",
    "print(f\"    variational ratio: {var_ratio.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainties of out-of-distribution example\n",
      "    mean prediction: 879, class probability: 0.20534050464630127\n",
      "    total predictive entropy: 3.4618284702301025\n",
      "    total mutual information: 0.4586506187915802\n",
      "    variational ratio: 0.7946594953536987\n"
     ]
    }
   ],
   "source": [
    "pred, pred_entropy, mutual_info, var_ratio = ensemble(ood_example)\n",
    "\n",
    "print(\"Uncertainties of out-of-distribution example\")\n",
    "print(f\"    mean prediction: {pred.argmax()}, class probability: {pred.max()}\")\n",
    "print(f\"    total predictive entropy: {pred_entropy.sum()}\")\n",
    "print(f\"    total mutual information: {mutual_info.sum()}\")\n",
    "print(f\"    variational ratio: {var_ratio.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainties of the random example\n",
      "    mean prediction: 735, class probability: 0.19145451486110687\n",
      "    total predictive entropy: 4.249647617340088\n",
      "    total mutual information: 0.22843754291534424\n",
      "    variational ratio: 0.8085454702377319\n"
     ]
    }
   ],
   "source": [
    "pred, pred_entropy, mutual_info, var_ratio = ensemble(random_example)\n",
    "\n",
    "print(\"Uncertainties of the random example\")\n",
    "print(f\"    mean prediction: {pred.argmax()}, class probability: {pred.max()}\")\n",
    "print(f\"    total predictive entropy: {pred_entropy.sum()}\")\n",
    "print(f\"    total mutual information: {mutual_info.sum()}\")\n",
    "print(f\"    variational ratio: {var_ratio.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of results\n",
    "\n",
    "We should see that in-distribution examples have a class probability close to 1 and a low uncertainties. Which means, we are predicting with high confidence.\n",
    "\n",
    "In the OOD and random sample we see that the prediction can be different everytime the ensemble is executed. This is due to the stochasticity of the MC dropout. \n",
    "\n",
    "Still, we can observe that the uncertainty is constantly very high, so we predict with low confidence. Whereas, the total predictive entropy (total uncertainty) is high for both, OOD and random example, we see that the mutual information is significantly different. High mutual information idicates that the total uncertainty is mainly due to model uncertainty. Whereas when the mutual information is low, it is an indicator that the total uncertainty is due to data uncertainty.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
